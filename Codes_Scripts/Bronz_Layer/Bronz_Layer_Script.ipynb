{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1c9cea0-e982-4a37-8c3e-8a0959a2c78d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# \uD83E\uDD49 Bronze Layer: Raw Data Ingestion\n",
    "\n",
    "## 1. Overview\n",
    "The **Bronze Layer** acts as the landing zone for all raw data. The primary goal is to capture the source data in its original state while adding basic metadata to track its entry into the Lakehouse.\n",
    "\n",
    "\n",
    "\n",
    "## 2. Ingestion Configuration Logic\n",
    "Instead of hard-coding every table, we use a **Metadata-Driven** approach. The `ingestion_config` list stores the necessary details for each file:\n",
    "\n",
    "* **`path`**: The source location within the **Unity Catalog Volume**.\n",
    "* **`table`**: The destination table name inside the `sales.bronz_layer` schema.\n",
    "\n",
    "### \uD83D\uDCCB Dataset Configuration\n",
    "The following list defines the files to be processed:\n",
    "\n",
    "| Source File Path | Target Table Name |\n",
    "| :--- | :--- |\n",
    "| `.../Sales.csv` | **Sales** |\n",
    "| `.../Products.csv` | **Products** |\n",
    "| `.../Customers.csv` | **Customers** |\n",
    "| `.../Stores_Locations.csv` | **Stores_Locations** |\n",
    "| `.../Regions.csv` | **Regions** |\n",
    "| `.../Sales_Teams.csv` | **Sales_Teams** |\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Key Ingestion Steps\n",
    "When the ingestion script runs, it performs the following automated tasks:\n",
    "\n",
    "1. **Read Source**: Loads files from the specified Volume paths.\n",
    "2. **Metadata Tagging**: Adds an `ingestion_data` column containing a timestamp.\n",
    "3. **Column Cleaning**: Automatically replaces spaces in headers with underscores (`_`) to satisfy **Delta Lake** requirements.\n",
    "4. **Delta Write**: Saves the data to the Catalog using `.saveAsTable()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ca3b849-5cd1-4ada-a171-24eae6c9f714",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ingestion_config = [\n",
    "  {\"path\":\"/Volumes/sales/bronz_layer/raw_data/Sales.csv\",\n",
    "   \"table\": \"Sales\"\n",
    "   },\n",
    "  {\"path\": \"/Volumes/sales/bronz_layer/raw_data/Products.csv\",\n",
    "   \"table\": \"Products\"\n",
    "   },\n",
    "  {\n",
    "    \"path\": \"/Volumes/sales/bronz_layer/raw_data/Customers.csv\",\n",
    "    \"table\": \"Customers\"\n",
    "  },\n",
    "  {\n",
    "    \"path\" : \"/Volumes/sales/bronz_layer/raw_data/Stores Locations.csv\",\n",
    "    \"table\": \"Stores_Locations\"\n",
    "  },\n",
    "  {\n",
    "    \"path\" : \"/Volumes/sales/bronz_layer/raw_data/Regions.csv\",\n",
    "    \"table\": \"Regions\"\n",
    "  },\n",
    "  {\n",
    "    \"path\": \"/Volumes/sales/bronz_layer/raw_data/Sales Teams.csv\",\n",
    "    \"table\": \"Sales_Teams\"\n",
    "  }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5dfb1b2e-9d61-4be2-be3e-3e620aa35fd7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trying to ingest : Sales\nsucess Sales is now in catalog\ntrying to ingest : Products\nsucess Products is now in catalog\ntrying to ingest : Customers\nsucess Customers is now in catalog\ntrying to ingest : Stores_Locations\nsucess Stores_Locations is now in catalog\ntrying to ingest : Regions\nsucess Regions is now in catalog\ntrying to ingest : Sales_Teams\nsucess Sales_Teams is now in catalog\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import datetime\n",
    "for item in ingestion_config : \n",
    "  try : \n",
    "    print(f\"trying to ingest : {item['table']}\")\n",
    "    df = pd.read_csv(item['path'], encoding= 'latin1') #we use different encoding, the product table contain some special characters\n",
    "    #replace spaces on columns names with '_'\n",
    "    df.columns = [c.replace(' ','_') for c in df.columns]\n",
    "    df['ingestion_data'] = datetime.datetime.now()\n",
    "    # convert pandas as to spark data frame \n",
    "    spark_df = spark.createDataFrame(df)\n",
    "    table_path = f\"sales.bronz_layer.{item['table']}\"\n",
    "    # save this spark dataframe as delta table \n",
    "    spark_df.write.mode(\"overwrite\").format(\"delta\").saveAsTable(table_path)\n",
    "    print(f\"sucess {item['table']} is now in catalog\")\n",
    "  except FileNotFoundError:\n",
    "        print(f\" Error: Could not find the file for {item['table']}.\")\n",
    "  except Exception as e:\n",
    "        print(f\" An error occurred: {e}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Bronz_Layer_Script",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}